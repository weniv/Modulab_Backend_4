# 1일차 (2025년 6월 12일)

## `.gitignore`에 추가

git 저장소에 추가(add)되지 않아야할 파일/디렉토리 이름 패턴들을 명시합니다. 명시된 이름들은 `git add` 시에 무시됩니다.

```
.env*
__pycache__
```

하지만, 이미 추가(add)된 파일들을 자동 제거해주지 않습니다.

제거할 파일들은 `git rm -r --cached 제거할파일경로` 명령으로 git 관리대상에서 제외시키되 `--cached` 옵션을 지정했기에 파일이 삭제되진 않습니다. 그리고 커밋해주시면 됩니다.

### `.env` 예시

환경변수 key/value를 정의합니다. 프로그램에서는 이 파일을 로딩해서 환경변수로서 활용합니다.

```
# .env 예시
#  등호(=) 앞 뒤로 절대 띄워쓰기가 있어서는 안 됩니다. .env 포맷에 위배되기에 그 줄은 무시됩니다.
OPENAI_API_KEY=...
DATABASE_URL=...
```

`.env` 파일을 활용하면, 하나의 파일에 다수의 환경변수를 등록하여 관리할 수 있기 때문에 편리합니다.

### 파이썬 코드에서 `.env`를 환경변수로서 로딩하기

+ `pip install -U python-dotenv`

```
from dotenv import load_dotenv
load_dotenv()
```

----

## OpenAI 라이브러리 사용해보기

### 위니브 지원 API

위니브 서버를 거쳐, OpenAI API 호출 (중계서버에서 API KEY 지정)

```python
import requests

def ask(messages: list[dict]) -> str:
    url = "https://..."  # API 주소 지정

    response = requests.post(url, json=messages, timeout=60)
    response.raise_for_status()  # 상태코드 200이 아니면 예외 발생

    response_obj: dict = response.json()
    input_tokens = response_obj["usage"]["prompt_tokens"]
    output_tokens = response_obj["usage"]["completion_tokens"]
    print("input : {}, output : {}", input_tokens, output_tokens)
    return response_obj["choices"][0]["message"]["content"]

def main():
    messages = [
        {
            "role": "system",
            "content": "...",  # TODO: 시스템 프롬프트 지정
        },
        {
            "role": "user",
            "content": "...",, # TODO: 유저 메시지 지정
        },
    ]
    ask(messages)
```

### LLM 공식 라이브러리 활용

+ [OpenAI 공식문서](https://platform.openai.com/docs/quickstart?api-mode=chat&lang=python)
    - 라이브러리 설치 : `pip install -U openai`

```python
from openai import OpenAI

api_key = "..."  # TODO: OpenAI API key 지정이 필요
client = OpenAI(api_key=api_key)

completion = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {
            "role": "user",
            "content": "hello"
        }
    ]
)

print(completion.choices[0].message.content)
```

+ [Anthropic 공식문서](https://github.com/anthropics/anthropic-sdk-python?tab=readme-ov-file#usage)
    - 라이브러리 설치 : `pip install -U openai`
+ [Google 공식문서](https://github.com/googleapis/python-genai?tab=readme-ov-file#generate-content)
    - 라이브러리 설치 : `pip install -U google-genai`
+ [Ollama 공식문서](https://github.com/ollama/ollama-python?tab=readme-ov-file#usage)
    - 라이브러리 설치 : `pip install -U ollama`

+ [업스테이지 공식문서](https://console.upstage.ai/docs/getting-started)
    - 라이브러리 설치 : `pip install -U openai` (OpenAI 호환)
    - 가입하면, 신용카드 등록없이도 $10 크레딧
    - [Solar Pro 2 Preview 모델이 7월 15일까지 무료](https://www.upstage.ai/blog/ko/solar-pro-2-preview-introduction)
    - 지원 모델 : [https://console.upstage.ai/docs/models](https://console.upstage.ai/docs/models)

```python
from openai import OpenAI

# https://console.upstage.ai/api-keys 에서 API Key 발급/확인
# https://console.upstage.ai/billing 에서 잔여 크레딧 확인
api_key = "up_******" # TODO: 업스테이지 API key 지정이 필요
client = OpenAI(
    # OpenAI 라이브러리이기에 생략하면 OPENAI_API_KEY 환경변수 참조
    # api_key=api_key,
    # 업스테이지 서버를 바라보도록 합니다.
    base_url="https://api.upstage.ai/v1",
)

completion = client.chat.completions.create(
    # model="solar-pro",
    model="solar-mini",
    messages=[
        {
            "role": "user",
            "content": "hello"
        }
    ]
)

print(completion.choices[0].message.content)
```

### 스트리밍 API 요청을 해보세요.

+ [OpenAI 공식문서](https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses#streaming)

### 선택 요청을 해보세요.

+ [OpenAI 공식문서](https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses)

### pyhub-llm 라이브러리 활용

+ 라이브러리 설치 : `pip install pyhub-llm[all]`

```python
from pyhub.llm import UpstageLLM
from pyhub.llm.types import Reply

llm = UpstageLLM()  # UPSTAGE_API_KEY 환경변수 활용
# llm = UpstageLLM(api_key="...")

reply: Reply = llm.ask("hello")
print(reply.text)
print(reply.usage)
```

수행 결과

```
Hello! How can I help you today? If you have any questions or need assistance, feel free to ask.
Usage(input=12, output=24)
```

#### Structured Output

```python
>>> from pyhub.llm import OpenAILLM
>>> llm = OpenAILLM(system_prompt="Extract entities from the input text")
>>> llm.ask("The quick brown for jumps over the lazy dog with piercing blue eyes")
# Reply(text='Entities extracted from the text:\n\n1. Quick brown fox\n2. Lazy dog\n3. Piercing blue eyes', usage=Usage(input=30, output=23), choice=None, choice_index=None, confidence=None, structured_data=None, validation_errors=None)
```

```python
from pydantic import BaseModel

class EntitiesModel(BaseModel):
    attributes: list[str]
    colors: list[str]
    animals: list[str]

>>> reply = llm.ask("The quick brown for jumps over the lazy dog with piercing blue eyes", schema=EntitiesModel)
>>> reply
# Reply(text='{"attributes":["quick","brown","lazy","piercing","blue"],"colors":["brown","blue"],"animals":["fox","dog"]}', usage=Usage(input=157, output=29), choice=None, choice_index=None, confidence=None, structured_data=EntitiesModel(attributes=['quick', 'brown', 'lazy', 'piercing', 'blue'], colors=['brown', 'blue'], animals=['fox', 'dog']), validation_errors=None)
>>> reply.structured_data
# EntitiesModel(attributes=['quick', 'brown', 'lazy', 'piercing', 'blue'], colors=['brown', 'blue'], animals=['fox', 'dog'])
```
